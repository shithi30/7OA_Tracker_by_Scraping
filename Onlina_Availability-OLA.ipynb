{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad52246b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping from location: Dhaka\n",
      "Scraping from location: Chattogram\n",
      "Scraping from location: Jashore\n",
      "Listings in result: 743\n",
      "Elapsed time to report (mins): 3.81\n"
     ]
    }
   ],
   "source": [
    "## Chaldal\n",
    "\n",
    "# import\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from googleapiclient.discovery import build\n",
    "from google.oauth2 import service_account\n",
    "import time\n",
    "\n",
    "# accumulators\n",
    "start_time = time.time()\n",
    "df_acc = pd.DataFrame()\n",
    "\n",
    "# list\n",
    "ubl_skus_df = pd.read_csv('Eagle Eye - Updated SKU List (All Platforms).csv')\n",
    "\n",
    "# preference\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('ignore-certificate-errors')\n",
    "\n",
    "# open window\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.maximize_window()\n",
    "\n",
    "# url\n",
    "i = 0\n",
    "while(1): \n",
    "    url = \"https://www.chaldal.com/Unilever\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # location\n",
    "    elem = driver.find_element(By.CLASS_NAME, \"metropolitanAreaName\")\n",
    "    elem.click()\n",
    "    elem = driver.find_element(By.ID, \"Group_47542\")\n",
    "    elem.click()\n",
    "    time.sleep(4)\n",
    "    elems = driver.find_elements(By.CLASS_NAME, \"cityImageContainer\")\n",
    "    achains = ActionChains(driver)\n",
    "    try: achains.move_to_element(elems[i]).click().perform()\n",
    "    except: break\n",
    "    time.sleep(6)\n",
    "    loc = driver.find_element(By.CLASS_NAME, \"metropolitanAreaName\").text.replace(\"\\n\", \" \")\n",
    "    print(\"Scraping from location: \" + loc)\n",
    "    i = i + 1\n",
    "\n",
    "    # scroll\n",
    "    SCROLL_PAUSE_TIME = 5\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height: break\n",
    "        last_height = new_height\n",
    "\n",
    "    # soup\n",
    "    soup_init = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    soup = soup_init.find_all(\"div\", attrs={\"class\": \"product\"})\n",
    "\n",
    "    # scrape\n",
    "    skus = []\n",
    "    quants = []\n",
    "    prices = []\n",
    "    prices_if_discounted = []\n",
    "    options = []\n",
    "    for s in soup:\n",
    "        # sku\n",
    "        try: val = s.find(\"div\", attrs={\"class\": \"name\"}).get_text()\n",
    "        except: val = None\n",
    "        skus.append(val)\n",
    "        # quantity\n",
    "        try: val = s.find(\"div\", attrs={\"class\": \"subText\"}).get_text() #.replace(\" \", \"\")\n",
    "        except: val = None\n",
    "        quants.append(val)\n",
    "        # price\n",
    "        try: val = float(s.find(\"div\", attrs={\"class\": \"price\"}).get_text().split()[1].replace(',', ''))\n",
    "        except: val = None\n",
    "        prices.append(val)\n",
    "        # discount\n",
    "        try: val = float(s.find(\"div\", attrs={\"class\": \"discountedPrice\"}).get_text().split()[1].replace(',', ''))\n",
    "        except: val = None\n",
    "        prices_if_discounted.append(val)\n",
    "        # option\n",
    "        try: val = s.find(\"p\", attrs={\"class\": \"buyText\"}).get_text() \n",
    "        except: val = None\n",
    "        options.append(val)\n",
    "\n",
    "    # accumulate\n",
    "    df = pd.DataFrame()\n",
    "    df['basepack'] = skus\n",
    "    df['sku'] = [str(s) + ' ' + str(q) for s, q in zip(skus, quants)]\n",
    "    df['quantity'] = quants\n",
    "    df['price'] = prices\n",
    "    df['price_if_discounted'] = prices_if_discounted\n",
    "    df['option'] = options\n",
    "    df['pos_in_pg'] = list(range(1, df.shape[0]+1))\n",
    "\n",
    "    # OOS\n",
    "    qry = '''\n",
    "    with \n",
    "        tbl3 as\n",
    "        (select * \n",
    "        from \n",
    "            (select Category category, Brand brand, \"Updated Perfect Name\" sku\n",
    "            from ubl_skus_df\n",
    "            where Platform='Chaldal'\n",
    "            ) tbl1 \n",
    "            left join \n",
    "            df tbl2 using(sku)\n",
    "        ) \n",
    "    select \n",
    "        *, \n",
    "        case \n",
    "            when category is not null and pos_in_pg is not null then 'enlisted + online'\n",
    "            when category is not null and pos_in_pg is null then 'enlisted + offline'\n",
    "            when category is null and pos_in_pg is not null then 'unlisted + online'\n",
    "            when category is null and pos_in_pg is null then 'unlisted + offline'\n",
    "        end ola_status\n",
    "    from \n",
    "        (select * from tbl3\n",
    "        union all \n",
    "        select null category, null brand, sku, basepack, quantity, price, price_if_discounted, option, pos_in_pg    \n",
    "        from df \n",
    "        where sku not in(select distinct sku from tbl3)\n",
    "        ) tbl4\n",
    "    '''\n",
    "    df = duckdb.query(qry).df()\n",
    "    df['location'] = loc\n",
    "    df['report_time'] = time.strftime('%d-%b-%y, %I:%M %p')\n",
    "    \n",
    "    # append\n",
    "    df_acc = df_acc._append(df)\n",
    "    \n",
    "    # wait\n",
    "    time.sleep(35)\n",
    "\n",
    "# close window\n",
    "driver.close()\n",
    "\n",
    "# csv\n",
    "df_acc.to_csv(\"chaldal_OLA_data.csv\", index=False)\n",
    "\n",
    "# credentials\n",
    "SERVICE_ACCOUNT_FILE = 'read-write-to-gsheet-apis-1-04f16c652b1e.json'\n",
    "SAMPLE_SPREADSHEET_ID = '1gkLRp59RyRw4UFds0-nNQhhWOaS4VFxtJ_Hgwg2x2A0'\n",
    "SCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "\n",
    "# APIs\n",
    "creds = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "service = build('sheets', 'v4', credentials=creds)\n",
    "sheet = service.spreadsheets()\n",
    "\n",
    "# update\n",
    "resultClear = sheet.values().clear(spreadsheetId=SAMPLE_SPREADSHEET_ID, range='Chaldal OLA').execute()\n",
    "qry = '''\n",
    "select\n",
    "    location, \n",
    "    count(category) skus_enlisted, \n",
    "    count(case when pos_in_pg is not null then category else null end) skus_online,\n",
    "    round(count(case when pos_in_pg is not null then category else null end)*1.00/count(category), 4) ola\n",
    "from df_acc\n",
    "where category is not null\n",
    "group by 1\n",
    "'''\n",
    "ola_df = duckdb.query(qry).df()\n",
    "ola_df['report_time'] = time.strftime('%d-%b-%y, %I:%M %p')\n",
    "request = sheet.values().update(spreadsheetId=SAMPLE_SPREADSHEET_ID, range=\"'Chaldal OLA'!N1\", valueInputOption='USER_ENTERED', body={'values': [ola_df.columns.values.tolist()] + ola_df.values.tolist()}).execute()\n",
    "df_acc = df_acc.fillna('')\n",
    "request = sheet.values().update(spreadsheetId=SAMPLE_SPREADSHEET_ID, range=\"'Chaldal OLA'!A1\", valueInputOption='USER_ENTERED', body={'values': [df_acc.columns.values.tolist()] + df_acc.values.tolist()}).execute()\n",
    "\n",
    "# stats\n",
    "# display(df_acc.head(5))\n",
    "print(\"Listings in result: \" + str(df_acc.shape[0]))\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time to report (mins): \" + str(round(elapsed_time / 60.00, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb283cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total SKUs found: 155\n",
      "Elapsed time to report (mins): 0.27\n"
     ]
    }
   ],
   "source": [
    "## Shajgoj\n",
    "\n",
    "# import\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from googleapiclient.discovery import build\n",
    "from google.oauth2 import service_account\n",
    "import time\n",
    "\n",
    "# accumulators\n",
    "start_time = time.time()\n",
    "\n",
    "# list\n",
    "ubl_skus_df = pd.read_csv('Eagle Eye - Updated SKU List (All Platforms).csv')\n",
    "\n",
    "# preference\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('ignore-certificate-errors')\n",
    "\n",
    "# open window\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.maximize_window()\n",
    "\n",
    "# url\n",
    "url = \"https://shop.shajgoj.com/unilever-bangladesh/\"\n",
    "driver.get(url)\n",
    "time.sleep(10)\n",
    "\n",
    "# soup\n",
    "soup_init = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "soup = soup_init.find_all(\"div\", attrs={\"class\": \"product_page shajgoj_upsell\"})\n",
    "\n",
    "# close window\n",
    "driver.close()\n",
    "\n",
    "# scrape\n",
    "skus = []\n",
    "quants = []\n",
    "prices = []\n",
    "prices_if_discounted = []\n",
    "offers = []\n",
    "options = []\n",
    "for s in soup:\n",
    "    # sku\n",
    "    try: val = s.find(\"div\", attrs={\"class\": \"upsell_name\"}).get_text()\n",
    "    except: val = None\n",
    "    skus.append(val)\n",
    "    # quantity\n",
    "    try: val = s.find(\"div\", attrs={\"class\": \"upsell_weight\"}).get_text()[0:-3].replace('(', '').replace(')', '')\n",
    "    except: val = None\n",
    "    quants.append(val)\n",
    "    # price\n",
    "    try: val = s.find(\"div\", attrs={\"class\": \"upsell_price\"}).get_text().split()[1]\n",
    "    except: val = None\n",
    "    prices.append(val)\n",
    "    # discount\n",
    "    try: val = s.find(\"div\", attrs={\"class\": \"upsell_price\"}).get_text().split()[3]\n",
    "    except: val = None\n",
    "    prices_if_discounted.append(val)\n",
    "    # offer\n",
    "    try: val = s.find(\"span\", attrs={\"class\": \"freq_sale_ribbon\"}).get_text()\n",
    "    except: val = None\n",
    "    offers.append(val)\n",
    "    # option\n",
    "    try: val = s.find(\"button\", attrs={\"type\": \"submit\"}).get_text()\n",
    "    except: val = s.find(\"button\", attrs={\"class\": \"request_restock product_recom\"}).get_text()\n",
    "    options.append(val)\n",
    "\n",
    "# accumulate\n",
    "df = pd.DataFrame()\n",
    "df['basepack'] = skus\n",
    "df['sku'] = [s + ' ' + q for s, q in zip(skus, quants)]\n",
    "df['quantity'] = quants\n",
    "df['price'] = prices\n",
    "df['price_if_discounted'] = prices_if_discounted\n",
    "df['pos_in_pg'] = list(range(1, df.shape[0]+1))\n",
    "df['offer'] = offers\n",
    "df['option'] = options\n",
    "df['ola_status'] = ['enlisted + online' if opt == 'ADD TO CART' else 'enlisted + offline' for opt in options]\n",
    "df['report_time'] = time.strftime('%d-%b-%y, %I:%M %p')\n",
    "\n",
    "# csv\n",
    "df.to_csv(\"shajgoj_OLA_data.csv\", index=False)\n",
    "\n",
    "# credentials\n",
    "SERVICE_ACCOUNT_FILE = 'read-write-to-gsheet-apis-1-04f16c652b1e.json'\n",
    "SAMPLE_SPREADSHEET_ID = '1gkLRp59RyRw4UFds0-nNQhhWOaS4VFxtJ_Hgwg2x2A0'\n",
    "SCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "\n",
    "# APIs\n",
    "creds = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "service = build('sheets', 'v4', credentials=creds)\n",
    "sheet = service.spreadsheets()\n",
    "\n",
    "# update\n",
    "resultClear = sheet.values().clear(spreadsheetId=SAMPLE_SPREADSHEET_ID, range='Shajgoj OLA').execute()\n",
    "qry = '''\n",
    "select\n",
    "    sum(1) skus_enlisted, \n",
    "    sum(case when option='ADD TO CART' then 1 else 0 end) skus_online,\n",
    "    round(sum(case when option='ADD TO CART' then 1 else 0 end)*1.00/sum(1), 4) ola\n",
    "from df\n",
    "'''\n",
    "ola_df = duckdb.query(qry).df()\n",
    "ola_df['report_time'] = time.strftime('%d-%b-%y, %I:%M %p')\n",
    "request = sheet.values().update(spreadsheetId=SAMPLE_SPREADSHEET_ID, range=\"'Shajgoj OLA'!L1\", valueInputOption='USER_ENTERED', body={'values': [ola_df.columns.values.tolist()] + ola_df.values.tolist()}).execute()\n",
    "df = df.fillna('')\n",
    "request = sheet.values().update(spreadsheetId=SAMPLE_SPREADSHEET_ID, range=\"'Shajgoj OLA'!A1\", valueInputOption='USER_ENTERED', body={'values': [df.columns.values.tolist()] + df.values.tolist()}).execute()\n",
    "\n",
    "# stats\n",
    "# display(df.head(5))\n",
    "print(\"Total SKUs found: \" + str(df.shape[0]))\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time to report (mins): \" + str(round(elapsed_time / 60.00, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daffeb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping from page: 1\n",
      "Scraping from page: 2\n",
      "Scraping from page: 3\n",
      "Scraping from page: 4\n",
      "Scraping from page: 5\n",
      "Scraping from page: 6\n",
      "Scraping from page: 7\n",
      "Scraping from page: 8\n",
      "Scraping from page: 9\n",
      "Scraping from page: 10\n",
      "Scraping from page: 11\n",
      "Scraping from page: 12\n",
      "Scraping from page: 13\n",
      "Scraping from page: 14\n",
      "Listings in result: 562\n",
      "Elapsed time to report (mins): 3.23\n"
     ]
    }
   ],
   "source": [
    "## Daraz\n",
    "\n",
    "# import\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from googleapiclient.discovery import build\n",
    "from google.oauth2 import service_account\n",
    "import time\n",
    "\n",
    "# accumulators\n",
    "start_time = time.time()\n",
    "df_acc = pd.DataFrame()\n",
    "\n",
    "# list\n",
    "ubl_skus_df = pd.read_csv('Eagle Eye - Updated SKU List (All Platforms).csv')\n",
    "\n",
    "# preference\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('ignore-certificate-errors')\n",
    "\n",
    "# open window\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.maximize_window()\n",
    "\n",
    "# link\n",
    "pg = 0\n",
    "while(1): \n",
    "    pg = pg + 1\n",
    "    link = \"https://www.daraz.com.bd/unilever-bangladesh/?from=wangpu&lang=en&langFlag=en&page=\" + str(pg) + \"&pageTypeId=2&q=All-Products\"\n",
    "    driver.get(link)\n",
    "\n",
    "    # scroll\n",
    "    SCROLL_PAUSE_TIME = 5\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height: break\n",
    "        last_height = new_height\n",
    "\n",
    "    # soup\n",
    "    soup_init = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    soup = soup_init.find_all(\"div\", attrs={\"class\": \"gridItem--Yd0sa\"})\n",
    "\n",
    "    # page\n",
    "    sku_count = len(soup)\n",
    "    if sku_count == 0: break \n",
    "    print(\"Scraping from page: \" + str(pg))\n",
    "    \n",
    "    # scrape\n",
    "    sku = []\n",
    "    current_price = []\n",
    "    original_price = []\n",
    "    offer = []\n",
    "    rating = []\n",
    "    reviews = []\n",
    "    in_mall = []\n",
    "    in_mart = []\n",
    "    pos_in_pg = []\n",
    "    for i in range(0, sku_count):\n",
    "        # SKU\n",
    "        try: val = soup[i].find(\"div\", attrs={\"id\": \"id-title\"}).get_text()\n",
    "        except: val = None\n",
    "        sku.append(val)\n",
    "        # current price\n",
    "        try: val = soup[i].find(\"span\", attrs={\"class\": \"currency--GVKjl\"}).get_text()\n",
    "        except: val = None\n",
    "        current_price.append(val)\n",
    "        # original price\n",
    "        try: val = soup[i].find(\"del\", attrs={\"class\": \"currency--GVKjl\"}).get_text()[2:]\n",
    "        except: val = None\n",
    "        original_price.append(val)\n",
    "        # offer\n",
    "        try: val = soup[i].find(\"div\", attrs={\"class\": \"voucher-wrapper--vCNzH\"}).get_text()\n",
    "        except: val = None\n",
    "        offer.append(val)\n",
    "        # rating    \n",
    "        try: val = soup[i].find(\"span\", attrs={\"class\": \"ratig-num--KNake rating--pwPrV\"}).get_text()\n",
    "        except: val = None\n",
    "        rating.append(val)\n",
    "        # reviews\n",
    "        try: val = soup[i].find(\"span\", attrs={\"class\": \"rating__review--ygkUy\"}).get_text()[1:-1]\n",
    "        except: val = None\n",
    "        reviews.append(val)\n",
    "        # mall\n",
    "        in_mall.append(1)\n",
    "        try: soup[i].find(\"i\", attrs={\"class\": \"ic-dynamic-badge ic-dynamic-badge-lazMall ic-dynamic-group-1\"})[\"style\"]\n",
    "        except: in_mall[i] = 0\n",
    "        # mart\n",
    "        in_mart.append(1)\n",
    "        try: soup[i].find(\"i\", attrs={\"class\": \"ic-dynamic-badge ic-dynamic-badge-redmart ic-dynamic-group-1\"})[\"style\"]\n",
    "        except: in_mart[i] = 0\n",
    "        # position\n",
    "        pos_in_pg.append(i+1)\n",
    "        \n",
    "    # accumulate \n",
    "    df = pd.DataFrame()\n",
    "    df['sku'] = sku\n",
    "    df['current_price'] = current_price\n",
    "    df['original_price'] = original_price\n",
    "    df['offer'] = offer\n",
    "    df['rating'] = rating\n",
    "    df['reviews'] = reviews\n",
    "    df['in_mall'] = in_mall\n",
    "    df['in_mart'] = in_mart\n",
    "    df['pg_no'] = pg\n",
    "    df['pos_in_pg'] = pos_in_pg\n",
    "    df_acc = df_acc._append(df)\n",
    "    \n",
    "# close window\n",
    "driver.close()\n",
    "\n",
    "# OOS\n",
    "qry = '''\n",
    "with \n",
    "    tbl3 as\n",
    "    (select * \n",
    "    from \n",
    "        (select Category category, Brand brand, \"Updated Perfect Name\" sku\n",
    "        from ubl_skus_df\n",
    "        where Platform='Daraz'\n",
    "        ) tbl1 \n",
    "        left join \n",
    "        df_acc tbl2 using(sku)\n",
    "    ) \n",
    "select \n",
    "    *, \n",
    "    case \n",
    "        when category is not null and pos_in_pg is not null then 'enlisted + online'\n",
    "        when category is not null and pos_in_pg is null then 'enlisted + offline'\n",
    "        when category is null and pos_in_pg is not null then 'unlisted + online'\n",
    "        when category is null and pos_in_pg is null then 'unlisted + offline'\n",
    "    end ola_status\n",
    "from \n",
    "    (select * from tbl3\n",
    "    union all \n",
    "    select null category, null brand, sku, current_price, original_price, offer, rating, reviews, in_mall, in_mart, pg_no, pos_in_pg\n",
    "    from df_acc\n",
    "    where sku not in(select distinct sku from tbl3)\n",
    "    ) tbl4\n",
    "'''\n",
    "df = duckdb.query(qry).df()\n",
    "df['location'] = 'Bangladesh'\n",
    "df['report_time'] = time.strftime('%d-%b-%y, %I:%M %p')\n",
    "\n",
    "# csv\n",
    "df.to_csv(\"daraz_OLA_data.csv\", index=False)\n",
    "\n",
    "# credentials\n",
    "SERVICE_ACCOUNT_FILE = 'read-write-to-gsheet-apis-1-04f16c652b1e.json'\n",
    "SAMPLE_SPREADSHEET_ID = '1gkLRp59RyRw4UFds0-nNQhhWOaS4VFxtJ_Hgwg2x2A0'\n",
    "SCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "\n",
    "# APIs\n",
    "creds = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "service = build('sheets', 'v4', credentials=creds)\n",
    "sheet = service.spreadsheets()\n",
    "\n",
    "# update\n",
    "resultClear = sheet.values().clear(spreadsheetId=SAMPLE_SPREADSHEET_ID, range='Daraz OLA').execute()\n",
    "qry = '''\n",
    "select\n",
    "    count(distinct sku) skus_enlisted, \n",
    "    count(distinct case when pos_in_pg is not null then sku else null end) skus_online,\n",
    "    round(count(distinct case when pos_in_pg is not null then sku else null end)*1.00/count(distinct sku), 4) ola\n",
    "from df\n",
    "where category is not null\n",
    "'''\n",
    "ola_df = duckdb.query(qry).df()\n",
    "ola_df['report_time'] = time.strftime('%d-%b-%y, %I:%M %p')\n",
    "request = sheet.values().update(spreadsheetId=SAMPLE_SPREADSHEET_ID, range=\"'Daraz OLA'!Q1\", valueInputOption='USER_ENTERED', body={'values': [ola_df.columns.values.tolist()] + ola_df.values.tolist()}).execute()\n",
    "df = df.fillna('')\n",
    "request = sheet.values().update(spreadsheetId=SAMPLE_SPREADSHEET_ID, range=\"'Daraz OLA'!A1\", valueInputOption='USER_ENTERED', body={'values': [df.columns.values.tolist()] + df.values.tolist()}).execute()\n",
    "\n",
    "# stats\n",
    "# display(df.head(5))\n",
    "print(\"Listings in result: \" + str(df.shape[0]))\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time to report (mins): \" + str(round(elapsed_time / 60.00, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ff16ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping from page: 1\n",
      "Scraping from page: 2\n",
      "Scraping from page: 3\n",
      "Listings in result: 172\n",
      "Elapsed time to report (mins): 0.87\n"
     ]
    }
   ],
   "source": [
    "## OHSOGO\n",
    "\n",
    "# import\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from googleapiclient.discovery import build\n",
    "from google.oauth2 import service_account\n",
    "import time\n",
    "\n",
    "# accumulators\n",
    "start_time = time.time()\n",
    "df_acc = pd.DataFrame()\n",
    "\n",
    "# list\n",
    "ubl_skus_df = pd.read_csv('Eagle Eye - Updated SKU List (All Platforms).csv')\n",
    "\n",
    "# preference\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('ignore-certificate-errors')\n",
    "\n",
    "# open window\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.maximize_window()\n",
    "\n",
    "# link\n",
    "pg = 0\n",
    "while(1):\n",
    "    pg = pg + 1  \n",
    "    link = \"https://ohsogo.com/collections/unilever?page=\" + str(pg)\n",
    "    driver.get(link)\n",
    "\n",
    "    # scroll\n",
    "    SCROLL_PAUSE_TIME = 5\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height: break\n",
    "        last_height = new_height\n",
    "\n",
    "    # soup\n",
    "    soup_init = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    soup = soup_init.find_all(\"div\", attrs={\"class\": \"card-wrapper card_space\"})\n",
    "    \n",
    "    # scrape\n",
    "    sku = []\n",
    "    current_price = []\n",
    "    original_price = []\n",
    "    offer = []\n",
    "    option = []\n",
    "    rating = []\n",
    "    brand = []\n",
    "    pos_in_pg = []\n",
    "    sku_count = len(soup)\n",
    "    for i in range(0, sku_count):\n",
    "        \n",
    "        # SKU\n",
    "        try: val = soup[i].find(\"a\", attrs={\"class\": \"card-information__text h4\"}).get_text().strip()\n",
    "        except: val = None\n",
    "        sku.append(val)\n",
    "\n",
    "        # current price\n",
    "        try: val = soup[i].find(\"span\", attrs={\"class\": \"price__style\"}).get_text().strip()\n",
    "        except: val = None\n",
    "        current_price.append(val)\n",
    "        \n",
    "        # original price\n",
    "        try: val = soup[i].find(\"bdi\", attrs={\"class\": \"maximun_price\"}).get_text()\n",
    "        except: val = None\n",
    "        original_price.append(val)\n",
    "    \n",
    "        # offer\n",
    "        try: val = soup[i].find(\"span\", attrs={\"class\": \"badge badge--onsale\"}).get_text().strip()\n",
    "        except: val = None\n",
    "        offer.append(val)\n",
    "        \n",
    "        # option\n",
    "        try: val = soup[i].find(\"div\", attrs={\"class\": \"card-information__button\"}).get_text().strip()\n",
    "        except: val = None\n",
    "        option.append(val)\n",
    "\n",
    "        # rating\n",
    "        try: val = soup[i].find(\"span\", attrs={\"class\": \"jdgm-prev-badge__stars\"})[\"data-score\"]\n",
    "        except: val = None\n",
    "        rating.append(val)\n",
    "        \n",
    "        # brand\n",
    "        try: val = soup[i].find(\"div\", attrs={\"class\": \"card-article-info caption-with-letter-spacing\"}).get_text()\n",
    "        except: val = None\n",
    "        brand.append(val)\n",
    "\n",
    "        # position\n",
    "        pos_in_pg.append(i+1)\n",
    "        \n",
    "    # page\n",
    "    if len(sku) == 0: break\n",
    "    print(\"Scraping from page: \" + str(pg))\n",
    "        \n",
    "    # accumulate \n",
    "    df = pd.DataFrame()\n",
    "    df['sku'] = sku\n",
    "    df['current_price'] = current_price\n",
    "    df['original_price'] = original_price\n",
    "    df['offer'] = offer\n",
    "    df['option'] = option\n",
    "    df['rating'] = rating\n",
    "    df['brand_scraped'] = brand\n",
    "    df['pg_no'] = pg\n",
    "    df['pos_in_pg'] = pos_in_pg\n",
    "    df_acc = df_acc._append(df)\n",
    "\n",
    "# close window\n",
    "driver.close()\n",
    "\n",
    "# OOS\n",
    "qry = '''\n",
    "with \n",
    "    tbl3 as\n",
    "    (select * \n",
    "    from \n",
    "        (select Category category, Brand brand, \"Updated Perfect Name\" sku\n",
    "        from ubl_skus_df\n",
    "        where Platform='Ohsogo'\n",
    "        ) tbl1 \n",
    "        left join \n",
    "        df_acc tbl2 using(sku)\n",
    "    ) \n",
    "select \n",
    "    *, \n",
    "    case \n",
    "        when category is not null and pos_in_pg is not null then 'enlisted + online'\n",
    "        when category is not null and pos_in_pg is null then 'enlisted + offline'\n",
    "        when category is null and pos_in_pg is not null then 'unlisted + online'\n",
    "        when category is null and pos_in_pg is null then 'unlisted + offline'\n",
    "    end ola_status\n",
    "from \n",
    "    (select * from tbl3\n",
    "    union all\n",
    "    select null category, null brand, sku, current_price, original_price, offer, option, rating, brand_scraped, pg_no, pos_in_pg\n",
    "    from df_acc\n",
    "    where sku not in(select distinct sku from tbl3)\n",
    "    ) tbl4\n",
    "'''\n",
    "df_acc = duckdb.query(qry).df()\n",
    "df_acc['report_time'] = time.strftime('%d-%b-%y, %I:%M %p')\n",
    "\n",
    "# csv\n",
    "df_acc.to_csv(\"ohsogo_OLA_data.csv\", index=False)\n",
    "\n",
    "# credentials\n",
    "SERVICE_ACCOUNT_FILE = 'read-write-to-gsheet-apis-1-04f16c652b1e.json'\n",
    "SAMPLE_SPREADSHEET_ID = '1gkLRp59RyRw4UFds0-nNQhhWOaS4VFxtJ_Hgwg2x2A0'\n",
    "SCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "\n",
    "# APIs\n",
    "creds = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "service = build('sheets', 'v4', credentials=creds)\n",
    "sheet = service.spreadsheets()\n",
    "\n",
    "# update\n",
    "resultClear = sheet.values().clear(spreadsheetId=SAMPLE_SPREADSHEET_ID, range='Ohsogo OLA').execute()\n",
    "qry = '''\n",
    "select\n",
    "    count(distinct sku) skus_enlisted, \n",
    "    count(distinct case when pos_in_pg is not null then sku else null end) skus_online,\n",
    "    round(count(distinct case when pos_in_pg is not null then sku else null end)*1.00/count(distinct sku), 4) ola\n",
    "from df_acc\n",
    "where category is not null\n",
    "'''\n",
    "ola_df = duckdb.query(qry).df()\n",
    "ola_df['report_time'] = time.strftime('%d-%b-%y, %I:%M %p')\n",
    "request = sheet.values().update(spreadsheetId=SAMPLE_SPREADSHEET_ID, range=\"'Ohsogo OLA'!O1\", valueInputOption='USER_ENTERED', body={'values': [ola_df.columns.values.tolist()] + ola_df.values.tolist()}).execute()\n",
    "df_acc = df_acc.fillna('')\n",
    "request = sheet.values().update(spreadsheetId=SAMPLE_SPREADSHEET_ID, range=\"'Ohsogo OLA'!A1\", valueInputOption='USER_ENTERED', body={'values': [df_acc.columns.values.tolist()] + df_acc.values.tolist()}).execute()\n",
    "\n",
    "# stats\n",
    "# display(df_acc.head(5))\n",
    "print(\"Listings in result: \" + str(df_acc.shape[0]))\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time to report (mins): \" + str(round(elapsed_time / 60.00, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d6521e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## email \n",
    "\n",
    "# import\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from googleapiclient.discovery import build\n",
    "from google.oauth2 import service_account\n",
    "import win32com.client\n",
    "from pretty_html_table import build_table\n",
    "import random\n",
    "import time\n",
    "\n",
    "# credentials\n",
    "SERVICE_ACCOUNT_FILE = 'read-write-to-gsheet-apis-1-04f16c652b1e.json'\n",
    "SAMPLE_SPREADSHEET_ID = '1gkLRp59RyRw4UFds0-nNQhhWOaS4VFxtJ_Hgwg2x2A0'\n",
    "SCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "\n",
    "# API\n",
    "creds = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "service = build('sheets', 'v4', credentials=creds)\n",
    "sheet = service.spreadsheets()\n",
    "\n",
    "# Chaldal\n",
    "values = sheet.values().get(spreadsheetId=SAMPLE_SPREADSHEET_ID, range='Chaldal OLA!N1:R').execute().get('values', [])\n",
    "ola_df_cldl = pd.DataFrame(values[1:] , columns = values[0])\n",
    "# Shajgoj\n",
    "values = sheet.values().get(spreadsheetId=SAMPLE_SPREADSHEET_ID, range='Shajgoj OLA!L1:O').execute().get('values', [])\n",
    "ola_df_shaj = pd.DataFrame(values[1:] , columns = values[0])\n",
    "# OHSOGO\n",
    "values = sheet.values().get(spreadsheetId=SAMPLE_SPREADSHEET_ID, range='OHSOGO OLA!O1:R').execute().get('values', [])\n",
    "ola_df_osgo = pd.DataFrame(values[1:] , columns = values[0])\n",
    "# Daraz\n",
    "values = sheet.values().get(spreadsheetId=SAMPLE_SPREADSHEET_ID, range='Daraz OLA!Q1:T').execute().get('values', [])\n",
    "ola_df_daaz = pd.DataFrame(values[1:] , columns = values[0])\n",
    "# Pandamart\n",
    "values = sheet.values().get(spreadsheetId=SAMPLE_SPREADSHEET_ID, range='Pandamart OLA!M1:Q').execute().get('values', [])\n",
    "ola_df_pmrt = pd.DataFrame(values[1:] , columns = values[0])\n",
    "\n",
    "# summary\n",
    "qry = '''\n",
    "select 'Chaldal' platform, location loc, skus_enlisted, skus_online, ola, report_time from ola_df_cldl\n",
    "union all\n",
    "select 'Daraz' platform, '-' loc, skus_enlisted, skus_online, ola, report_time from ola_df_daaz\n",
    "union all\n",
    "select 'Shajgoj' platform, '-' loc, skus_enlisted, skus_online, ola, report_time from ola_df_shaj\n",
    "union all\n",
    "select 'OHSOGO' platform, '-' loc, skus_enlisted, skus_online, ola, report_time from ola_df_osgo \n",
    "union all\n",
    "select 'Pandamart' platform, site loc, skus_enlisted, skus_online, concat(rpad(left((ola::float*100)::text, 5), 5, '0'), '%') ola, report_time from ola_df_pmrt where report_time like ''' + \"'\" + time.strftime('%d-%b-%y') + \"%'\"\n",
    "ola_email_df = duckdb.query(qry).df()\n",
    "ola_email_df.columns = ['Platform', 'Location', 'SKUs Enlisted', 'SKUs Online', 'OLA', 'Report Time']\n",
    "\n",
    "# email\n",
    "ol = win32com.client.Dispatch(\"outlook.application\")\n",
    "olmailitem = 0x0\n",
    "newmail = ol.CreateItem(olmailitem)\n",
    "\n",
    "# subject, recipients\n",
    "newmail.Subject = 'OLA Status ' + time.strftime('%d-%b-%y')\n",
    "newmail.To = \"avra.barua@unilever.com; safa-e.nafee@unilever.com; rafid-al.mahmood@unilever.com; anulekha.chowdhuri2@unilever.com\"\n",
    "newmail.BCC = \"shithi30@outlook.com\"\n",
    "\n",
    "# body\n",
    "newmail.HTMLbody = f'''\n",
    "Dear concern,<br><br>\n",
    "As part of <i>Eagle Eye</i>'s 7OA monitoring, this email summarizes today's OLA as below. View full results <a href=\"https://docs.google.com/spreadsheets/d/1gkLRp59RyRw4UFds0-nNQhhWOaS4VFxtJ_Hgwg2x2A0/edit#gid=646361614\">here</a>.\n",
    "''' + build_table(ola_email_df, random.choice(['green_dark', 'red_dark', 'blue_dark', 'grey_dark', 'orange_dark']), font_size='12px', text_align='left') + '''\n",
    "Note that, the statistics presented are reflections at the time of scraping. This is an auto email via <i>win32com</i>.<br><br>\n",
    "Thanks,<br>\n",
    "Shithi Maitra<br>\n",
    "Asst. Manager, CSE<br>\n",
    "Unilever BD Ltd.<br>\n",
    "'''\n",
    "# send\n",
    "newmail.Send()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bca9cf8-ca84-412d-b4c6-57b33523134c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
